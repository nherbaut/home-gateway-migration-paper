\begin{table*}
	\centering
	\begin{tabular}{| c | p{0.25\textwidth}|c |c |c || c |c |c |}
	
	
 	    \multicolumn{2}{c}{} & \multicolumn{3}{c}{Web Traffic} 		  & \multicolumn{3}{c}{File Transfer} \\
 	     \cline{3-8}	
             \multicolumn{2}{c|}{} & CPU 			& Memory (Mb) 		& Throughput (Mbps)	& CPU 		& Memory (Mb)		& Throughput (Mbps) \\\hline   
Settings 1 & client connected to the gateway with ip forwarding &   3.6\% 		& 693 		& 11.461		& 2.7\%		& 708 Mb		& 11.455 \\\hline
Settings 2 & client connected via squid proxy hosted on the gateway   &   52\%        & 692 		& 11.455		& 47\%		& 703 Mb		& 11.450 \\\hline
Settings 3 & client connected to SvNF proxy hosted on the gateway with no rule deployed &   66\%		& 680 		& 11.404		& 54\%		& 707 Mb		& 11.449 \\\hline
Settings 4 & client connected to SvNF proxy hosted on the gateway with 10,000 rules deployed   &   69\%        & 690 		& 11.430		& 56\%		& 704 Mb		& 11.444 \\\hline

	
	
	            
	\end{tabular}
	\caption{
	OSGi HTTP proxy performance comparison
	\label{tab:perf-comparison}
	}
	
\end{table*}

Section~\ref{sec:migrating} described the role of SvNF deployed on the HG and the server-side infrastructure composed of various vNF: Streamers deployed in POPS, Caching and Transcoding orchestrator and Transcoding vNF deployed in the core.
As our proposal aims at showing how software deployed in a modular HG can play a role in a vNF architecture, our focus for the evaluation is devoted to assessing HG side performance, memory footprint and environement execution. In a future work, we will also asses server-side operations.


\subsection{Testbed}

For the testbed, we used commodity hardware along with 100 Mb Ethernet network. As software, Ubuntu 14.04 for hypervisors and Debian 7 for the VM and gateway were used. Workers were started throughout the experiment.
  
For performance assessments, we made sure that remote resources awould be deployed in the most stable environment possible. This helped us ruling out possible network noise during our test procedures. We used a dedicated network attached storage connected to the same access point which provided us with very stable performances and low latencyto deploy video resources and file. We also included the results from fetching resources from a CDN reached through WAN.

 \subsubsection{Testing Methodology}
JMeter\footnote{http://jmeter.apache.org} was used to capture the network metrics of our solution.
It allowed us to create agents and make them perform standard HTTP and report on specific performances metrics.    

The HG overhead can be decomposed in several pieces.
It can be caused by the hardware or software and can consist of increased latency and longer packet processing.
We decided to use the simple approach of measuring the overal througput 

First, we considered downloading a video file, then a single HTML page which links to several other static resources like Javascript files, CSS or images. 
We decided that the best and simplest way to represent the impact of the SvNF on the performance was to measure the throughput for different scenario. In the first scenario, a 20 Mb video file is downloaded, for the second scenario, a HTML page is downloaded along with 171 other attached resources of average size 16 kb. It is clear that the first scenario result will reflect the impact on the download time while the other will be much more sensitive to latency, due to the amount of smaller resources to download.

We evaluated our solution with two different rules settings deployed in the SvNF. In the first one, we didn't deploy any rules, assessing only the overhead linked to the application network framework. In the second one however, we deployed 10.000 rules, causing our bundle to process both requests and responses wrt those rules thereby assessing its ability to perform pattern matching.

We developed an OSGi Bundle deployed on the HG over Apache Karaf/Felix, a popular OSGi runtime.

\subsection{Results}

Hardware overhead can be assessed by comparing the throughput from "Direct Connection" where the host is directly connected to the WAN access point to the "IP forwarding" where we configured the gateway to perform IP forwarding at kernel level. The hardware overhead is minimal, generally less than 7\%.
We decided to assess the overhead caused the system by comparing a well-known HTTP proxy called Squid3 to our solution under different settings. To have a better grasp of the amont of resource consumed by our solution, we also reported CPU and memory consumption for each settings as well.
We can see that our solution competes well with Squid3, since the maximum penalty is 11\%. More interestingly, we can see that our solution is not overly penalized by the rules installed in the vHG as the maximum penalty is 5 \% wrt the performances of the system without any rule. CPU is also affected up to 9 percentage points.


% Please add the following required packages to your document preamble:



